{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you have all libraries installed before start the lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requests:\n",
    "\"Requests is one of the most downloaded Python packages of all time, pulling in over 400,000 downloads each day. Join the party!\"\n",
    "\n",
    "[Source](https://2.python-requests.org/en/master/)\n",
    "\n",
    "##### internal jokes:\n",
    "\"Requests is the only Non-GMO HTTP library for Python, safe for human consumption.\" #3556\n",
    "[Source](https://github.com/kennethreitz/requests/issues/3556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = soup.find_all('h1',class_=\"h3 lh-condensed\")\n",
    "\n",
    "name_t = [name1[i].get_text().replace(\"\\n\",\"\").strip() for i in range(len(name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['文翼',\n",
       " 'Daniel Wirtz',\n",
       " 'Arvid Norberg',\n",
       " 'Ran Luo',\n",
       " 'James Newton-King',\n",
       " 'Sebastián Ramírez',\n",
       " 'XhmikosR',\n",
       " 'Adam Scarr',\n",
       " 'Lipis',\n",
       " 'Lee Dohm',\n",
       " 'holger krekel',\n",
       " 'Benjamin Peterson',\n",
       " 'Manu MA',\n",
       " 'Niels Lohmann',\n",
       " 'Andrew Svetlov',\n",
       " 'Krzysztof Magiera',\n",
       " 'Swapnil Agarwal',\n",
       " 'Jeffrey Su',\n",
       " 'Jack Lloyd',\n",
       " 'Mr.doob',\n",
       " 'David Desmaisons',\n",
       " 'Rakan Nimer',\n",
       " 'Sebastian Raschka',\n",
       " 'Stefan Prodan',\n",
       " 'Ben McCann']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "import requests\n",
    "page2 = requests.get(url2)\n",
    "\n",
    "soup2 = BeautifulSoup(page2.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['axi0mX (ipwndfu)',\n",
       " 'svip-lab (impersonator)',\n",
       " 'geekcomputers (Python)',\n",
       " 'brightmart (albert_zh)',\n",
       " '521xueweihan (HelloGitHub)',\n",
       " 'huggingface (transformers)',\n",
       " 'SQRPI (JiaGuoMeng)',\n",
       " 'nvbn (thefuck)',\n",
       " 'pielco11 (fav-up)',\n",
       " 'CoreyMSchafer (code_snippets)',\n",
       " 'testerSunshine (12306)',\n",
       " 'donnemartin (system-design-primer)',\n",
       " 'Avik-Jain (100-Days-Of-ML-Code)',\n",
       " 'tie1r1 (checkm8gui)',\n",
       " 'pallets (flask)',\n",
       " 'anishathalye (neural-style)',\n",
       " 'ytdl-org (youtube-dl)',\n",
       " 'ildoonet (tf-pose-estimation)',\n",
       " 'Nyloner (Nyspider)',\n",
       " 'wangzheng0822 (algo)',\n",
       " 'twintproject (twint)',\n",
       " 'hamuchiwa (AutoRCCar)',\n",
       " 'Kr1s77 (awesome-python-login-model)',\n",
       " 'microsoft (c9-python-getting-started)',\n",
       " 'pi-hole (docker-pi-hole)']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2 = soup2.find_all('h1',class_=\"h3 lh-condensed\")\n",
    "\n",
    "name2_t = [name2[i].get_text().replace(\"\\n\",\"\").replace(\"/      \",\"(\").strip() + ')' for i in range(len(name))]\n",
    "name2_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png)',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png)',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1)',\n",
       " '/static/images/wikimedia-button.png)',\n",
       " '/static/images/poweredby_mediawiki_88x31.png)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "\n",
    "import urllib.request  as urllib2 \n",
    "\n",
    "soup3 = BeautifulSoup(urllib2.urlopen(url3).read())\n",
    "name3 = soup3.find_all('img')\n",
    "\n",
    "name3_img = [name3[i].get('src').replace(\"\\n\",\"\").replace(\"/      \",\"(\").strip() + ')' for i in range(len(name3))]\n",
    "name3_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "\n",
    "import requests\n",
    "page4 = requests.get(url4)\n",
    "\n",
    "soup4 = BeautifulSoup(page4.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#p-search\n",
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "#Snakes\n",
      "#Ancient_Greece\n",
      "#Media_and_entertainment\n",
      "#Computing\n",
      "#Engineering\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#People\n",
      "#Other_uses\n",
      "#See_also\n",
      "/w/index.php?title=Python&action=edit&section=1\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/w/index.php?title=Python&action=edit&section=2\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/w/index.php?title=Python&action=edit&section=3\n",
      "/wiki/Python_(film)\n",
      "/wiki/Pythons_2\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/w/index.php?title=Python&action=edit&section=4\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CPython\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/w/index.php?title=Python&action=edit&section=5\n",
      "/w/index.php?title=Python&action=edit&section=6\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(Efteling)\n",
      "/w/index.php?title=Python&action=edit&section=7\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/w/index.php?title=Python&action=edit&section=8\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/w/index.php?title=Python&action=edit&section=9\n",
      "/wiki/Python_Anghelo\n",
      "/w/index.php?title=Python&action=edit&section=10\n",
      "/wiki/PYTHON\n",
      "/w/index.php?title=Python&action=edit&section=11\n",
      "/wiki/Cython\n",
      "/wiki/Pyton\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=918851926\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_short_description\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Special:MyContributions\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "/wiki/Main_Page\n",
      "/wiki/Main_Page\n",
      "/wiki/Portal:Contents\n",
      "/wiki/Portal:Featured_content\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "//shop.wikimedia.org\n",
      "/wiki/Help:Contents\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Python&oldid=918851926\n",
      "/w/index.php?title=Python&action=info\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452\n",
      "/w/index.php?title=Special:CiteThisPage&page=Python&id=918851926\n",
      "https://commons.wikimedia.org/wiki/Category:Python\n",
      "/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Python\n",
      "/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen\n",
      "/w/index.php?title=Python&printable=yes\n",
      "https://af.wikipedia.org/wiki/Python\n",
      "https://als.wikipedia.org/wiki/Python\n",
      "https://az.wikipedia.org/wiki/Python\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\n",
      "https://be.wikipedia.org/wiki/Python\n",
      "https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\n",
      "https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\n",
      "https://da.wikipedia.org/wiki/Python\n",
      "https://de.wikipedia.org/wiki/Python\n",
      "https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\n",
      "https://fr.wikipedia.org/wiki/Python\n",
      "https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\n",
      "https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "https://io.wikipedia.org/wiki/Pitono\n",
      "https://id.wikipedia.org/wiki/Python\n",
      "https://ia.wikipedia.org/wiki/Python_(disambiguation)\n",
      "https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\n",
      "https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\n",
      "https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "https://lb.wikipedia.org/wiki/Python\n",
      "https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\n",
      "https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\n",
      "https://nl.wikipedia.org/wiki/Python\n",
      "https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Pyton\n",
      "https://pl.wikipedia.org/wiki/Pyton\n",
      "https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\n",
      "https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\n",
      "https://sd.wikipedia.org/wiki/%D8%A7%D8%B1%DA%99\n",
      "https://sk.wikipedia.org/wiki/Python\n",
      "https://sh.wikipedia.org/wiki/Python\n",
      "https://fi.wikipedia.org/wiki/Python\n",
      "https://sv.wikipedia.org/wiki/Pyton\n",
      "https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Python\n",
      "https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\n",
      "https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\n",
      "https://vi.wikipedia.org/wiki/Python\n",
      "https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "for link in soup4.findAll(\"a\"):\n",
    "    if 'href' in link.attrs:\n",
    "         print(link.attrs['href'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/wiki/Python_(film)\n",
      "/wiki/Pythons_2\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CPython\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(Efteling)\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/wiki/Python_Anghelo\n",
      "/wiki/Cython\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n"
     ]
    }
   ],
   "source": [
    "link4=[]\n",
    "\n",
    "for link in soup4.find_all('li'):\n",
    "    if link.find('a') is not None: \n",
    "        if link.find('a').get('href').startswith('/wiki/') and 'ython' in link.find('a').get('href'):\n",
    "            print(link.find('a').get('href'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'http://uscode.house.gov/download/download.shtml'\n",
    "import requests\n",
    "page5 = requests.get(url5)\n",
    "\n",
    "soup5 = BeautifulSoup(page5.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All titles in the format selected compressed into a zip archive.',\n",
       " 'Title 1  General Provisions',\n",
       " 'Title 2  The Congress',\n",
       " 'Title 3  The President',\n",
       " 'Title 4  Flag and Seal, Seat of Government, and the States',\n",
       " 'Title 5  Government Organization and Employees',\n",
       " '',\n",
       " 'Title 6  Domestic Security',\n",
       " 'Title 7  Agriculture',\n",
       " 'Title 8  Aliens and Nationality',\n",
       " 'Title 9  Arbitration',\n",
       " 'Title 10  Armed Forces',\n",
       " 'Title 11  Bankruptcy',\n",
       " '',\n",
       " 'Title 12  Banks and Banking',\n",
       " 'Title 13  Census',\n",
       " 'Title 14  Coast Guard',\n",
       " 'Title 15  Commerce and Trade',\n",
       " 'Title 16  Conservation',\n",
       " 'Title 17  Copyrights',\n",
       " 'Title 18  Crimes and Criminal Procedure',\n",
       " '',\n",
       " 'Title 19  Customs Duties',\n",
       " 'Title 20  Education',\n",
       " 'Title 21  Food and Drugs',\n",
       " 'Title 22  Foreign Relations and Intercourse',\n",
       " 'Title 23  Highways',\n",
       " 'Title 24  Hospitals and Asylums',\n",
       " 'Title 25  Indians',\n",
       " 'Title 26  Internal Revenue Code',\n",
       " 'Title 27  Intoxicating Liquors',\n",
       " 'Title 28  Judiciary and Judicial Procedure',\n",
       " '',\n",
       " 'Title 29  Labor',\n",
       " 'Title 30  Mineral Lands and Mining',\n",
       " 'Title 31  Money and Finance',\n",
       " 'Title 32  National Guard',\n",
       " 'Title 33  Navigation and Navigable Waters',\n",
       " 'Title 34  Crime Control and Law Enforcement',\n",
       " 'Title 35  Patents',\n",
       " 'Title 36  Patriotic and National Observances, Ceremonies, and Organizations',\n",
       " 'Title 37  Pay and Allowances of the Uniformed Services',\n",
       " \"Title 38  Veterans' Benefits\",\n",
       " 'Title 39  Postal Service',\n",
       " 'Title 40  Public Buildings, Property, and Works',\n",
       " 'Title 41  Public Contracts',\n",
       " 'Title 42  The Public Health and Welfare',\n",
       " 'Title 43  Public Lands',\n",
       " 'Title 44  Public Printing and Documents',\n",
       " 'Title 45  Railroads',\n",
       " 'Title 46  Shipping',\n",
       " 'Title 47  Telecommunications',\n",
       " 'Title 48  Territories and Insular Possessions',\n",
       " 'Title 49  Transportation',\n",
       " 'Title 50  War and National Defense',\n",
       " '',\n",
       " 'Title 51  National and Commercial Space Programs',\n",
       " 'Title 52  Voting and Elections',\n",
       " 'Title 53 [Reserved]',\n",
       " 'Title 54  National Park Service and Related Programs']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name5 = soup5.find_all('div', class_=\"uscitem\")\n",
    "\n",
    "name5_t = [name5[i].get_text().replace(\"\\n\",\"\").replace(\"*\",\"\").replace(\"116\",\"\").replace(\"57\",\"\").replace(\"-\",\"\").replace(\"٭\",\"\").replace(\"Appendix\",\"\").replace(\"[XML]\\xa0[XHTML]\\xa0[PCC]\\xa0[PDF]\",\"\").strip() for i in range(len(name5))]\n",
    "name5_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://www.fbi.gov/wanted/topten'\n",
    "import requests\n",
    "page6 = requests.get(url6)\n",
    "\n",
    "soup6 = BeautifulSoup(page6.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name6 = soup6.find_all('h3', {'class' : 'title'})\n",
    "\n",
    "name6_t = [name6[i].text.replace(\"\\n\",\"\") for i in range(len(name6))]\n",
    "\n",
    "\n",
    "#name2_t = [name2[i].get_text().replace(\"\\n\",\"\").replace(\"/      \",\"(\").strip() + ')' for i in range(len(name))]\n",
    "name6_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost there -- the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "import requests\n",
    "page7 = requests.get(url7)\n",
    "\n",
    "soup7 = BeautifulSoup(page7.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = soup7.find_all('a')\n",
    "lat_long = soup7.find_all('td', {'class':\"tabev1\"})\n",
    "region_name = soup7.find_all('td', {'class':'tb_region'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name_t = [region_name[i].text.replace(\"\\xa0\",\"\") for i in range(len(region_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = soup7.find_all('td', {'class':\"tabev1\"})\n",
    "\n",
    "long_t=[]\n",
    "lat_t=[]\n",
    "\n",
    "for i in range(len(lat_long)):\n",
    "    if i % 2 == 0:\n",
    "        lat_t.append(lat_long[i].text.replace(\"\\xa0\",\"\"))\n",
    "    else:\n",
    "        long_t.append(lat_long[i].text.replace(\"\\xa0\",\"\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['earthquake2019-10-01', '17:20:27.734min', 'ago'],\n",
       " ['earthquake2019-10-01', '16:32:44.51hr', '22min', 'ago'],\n",
       " ['earthquake2019-10-01', '16:29:23.51hr', '25min', 'ago'],\n",
       " ['earthquake2019-10-01', '16:24:38.11hr', '30min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:51:44.02hr', '03min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:45:56.32hr', '09min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:34:06.22hr', '21min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:19:23.02hr', '35min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:09:45.72hr', '45min', 'ago'],\n",
       " ['earthquake2019-10-01', '15:08:41.02hr', '46min', 'ago'],\n",
       " ['earthquake2019-10-01', '14:35:46.03hr', '19min', 'ago'],\n",
       " ['earthquake2019-10-01', '14:34:38.63hr', '20min', 'ago'],\n",
       " ['earthquake2019-10-01', '14:33:54.23hr', '21min', 'ago'],\n",
       " ['earthquake2019-10-01', '14:15:37.13hr', '39min', 'ago'],\n",
       " ['earthquake2019-10-01', '14:02:19.93hr', '52min', 'ago'],\n",
       " ['earthquake2019-10-01', '13:35:41.84hr', '19min', 'ago'],\n",
       " ['earthquake2019-10-01', '13:11:08.84hr', '44min', 'ago'],\n",
       " ['earthquake2019-10-01', '12:32:48.75hr', '22min', 'ago'],\n",
       " ['earthquake2019-10-01', '12:22:28.25hr', '32min', 'ago'],\n",
       " ['earthquake2019-10-01', '12:09:47.05hr', '45min', 'ago'],\n",
       " ['earthquake2019-10-01', '11:51:47.26hr', '03min', 'ago'],\n",
       " ['earthquake2019-10-01', '11:42:54.16hr', '12min', 'ago'],\n",
       " ['earthquake2019-10-01', '11:26:58.76hr', '28min', 'ago'],\n",
       " ['earthquake2019-10-01', '11:20:08.76hr', '35min', 'ago'],\n",
       " ['earthquake2019-10-01', '11:06:25.06hr', '48min', 'ago'],\n",
       " ['earthquake2019-10-01', '10:57:18.06hr', '58min', 'ago'],\n",
       " ['earthquake2019-10-01', '10:52:01.97hr', '03min', 'ago'],\n",
       " ['earthquake2019-10-01', '10:33:31.77hr', '21min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:54:35.08hr', '00min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:52:58.78hr', '02min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:46:40.08hr', '08min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:45:25.48hr', '09min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:29:20.48hr', '25min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:23:30.08hr', '31min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:15:46.88hr', '39min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:12:44.08hr', '42min', 'ago'],\n",
       " ['earthquake2019-10-01', '09:00:34.18hr', '54min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:53:05.99hr', '02min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:52:07.99hr', '03min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:44:49.09hr', '10min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:42:28.79hr', '12min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:29:56.09hr', '25min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:28:31.49hr', '26min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:27:03.09hr', '28min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:21:50.99hr', '33min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:17:02.29hr', '38min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:10:30.09hr', '44min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:08:40.79hr', '46min', 'ago'],\n",
       " ['earthquake2019-10-01', '08:01:33.09hr', '53min', 'ago'],\n",
       " ['earthquake2019-10-01', '07:55:27.99hr', '59min', 'ago']]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time = soup7.find_all('td', {'class':\"tabev6\"})\n",
    "\n",
    "date_time_t = [date_time[i].text.split() for i in range(len(date_time))]\n",
    "\n",
    "date_time_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_t1=[]\n",
    "time_t=[]\n",
    "\n",
    "for i in range(len(date_time_t)):\n",
    "    date_t1.append(date_time_t[i][0])\n",
    "    time_t.append(date_time_t[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01', '2019-10-01']\n"
     ]
    }
   ],
   "source": [
    "date_t=[]\n",
    "\n",
    "for i in range(len(date_t1)):\n",
    "    date_t.append(date_t1[i].split('earthquake')[1])\n",
    "\n",
    "print(date_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "        'data': (date_t[0:20]),\n",
    "        'time': (time_t[0:20]),\n",
    "        'latitude': (lat_t[0:20]),\n",
    "        'longitude': (long_t[0:20]),\n",
    "        'regiao': (region_name_t[0:20])\n",
    "}\n",
    "    \n",
    "    \n",
    "df_info = pd.DataFrame(data, columns = ['data', 'time','latitude','longitude','regiao'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>regiao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>17:20:27.734min</td>\n",
       "      <td>38.78</td>\n",
       "      <td>27.38</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>16:32:44.51hr</td>\n",
       "      <td>61.76</td>\n",
       "      <td>149.87</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>16:29:23.51hr</td>\n",
       "      <td>47.38</td>\n",
       "      <td>120.96</td>\n",
       "      <td>WASHINGTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>16:24:38.11hr</td>\n",
       "      <td>35.92</td>\n",
       "      <td>30.50</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:51:44.02hr</td>\n",
       "      <td>18.54</td>\n",
       "      <td>69.64</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:45:56.32hr</td>\n",
       "      <td>35.91</td>\n",
       "      <td>98.52</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:34:06.22hr</td>\n",
       "      <td>40.87</td>\n",
       "      <td>28.30</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:19:23.02hr</td>\n",
       "      <td>15.96</td>\n",
       "      <td>98.44</td>\n",
       "      <td>OFFSHORE OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:09:45.72hr</td>\n",
       "      <td>34.79</td>\n",
       "      <td>97.50</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>15:08:41.02hr</td>\n",
       "      <td>33.49</td>\n",
       "      <td>116.80</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>14:35:46.03hr</td>\n",
       "      <td>3.43</td>\n",
       "      <td>128.32</td>\n",
       "      <td>SERAM, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>14:34:38.63hr</td>\n",
       "      <td>35.80</td>\n",
       "      <td>96.66</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>14:33:54.23hr</td>\n",
       "      <td>38.20</td>\n",
       "      <td>40.99</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>14:15:37.13hr</td>\n",
       "      <td>34.80</td>\n",
       "      <td>97.47</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>14:02:19.93hr</td>\n",
       "      <td>19.28</td>\n",
       "      <td>155.49</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>13:35:41.84hr</td>\n",
       "      <td>36.69</td>\n",
       "      <td>97.69</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>13:11:08.84hr</td>\n",
       "      <td>35.75</td>\n",
       "      <td>117.59</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>12:32:48.75hr</td>\n",
       "      <td>35.66</td>\n",
       "      <td>117.47</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>12:22:28.25hr</td>\n",
       "      <td>41.35</td>\n",
       "      <td>19.57</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>12:09:47.05hr</td>\n",
       "      <td>37.42</td>\n",
       "      <td>20.65</td>\n",
       "      <td>IONIAN SEA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          data             time latitude longitude                     regiao\n",
       "0   2019-10-01  17:20:27.734min    38.78     27.38             WESTERN TURKEY\n",
       "1   2019-10-01    16:32:44.51hr    61.76    149.87            SOUTHERN ALASKA\n",
       "2   2019-10-01    16:29:23.51hr    47.38    120.96                 WASHINGTON\n",
       "3   2019-10-01    16:24:38.11hr    35.92     30.50  EASTERN MEDITERRANEAN SEA\n",
       "4   2019-10-01    15:51:44.02hr    18.54     69.64         DOMINICAN REPUBLIC\n",
       "5   2019-10-01    15:45:56.32hr    35.91     98.52                   OKLAHOMA\n",
       "6   2019-10-01    15:34:06.22hr    40.87     28.30             WESTERN TURKEY\n",
       "7   2019-10-01    15:19:23.02hr    15.96     98.44    OFFSHORE OAXACA, MEXICO\n",
       "8   2019-10-01    15:09:45.72hr    34.79     97.50                   OKLAHOMA\n",
       "9   2019-10-01    15:08:41.02hr    33.49    116.80        SOUTHERN CALIFORNIA\n",
       "10  2019-10-01    14:35:46.03hr     3.43    128.32           SERAM, INDONESIA\n",
       "11  2019-10-01    14:34:38.63hr    35.80     96.66                   OKLAHOMA\n",
       "12  2019-10-01    14:33:54.23hr    38.20     40.99             EASTERN TURKEY\n",
       "13  2019-10-01    14:15:37.13hr    34.80     97.47                   OKLAHOMA\n",
       "14  2019-10-01    14:02:19.93hr    19.28    155.49   ISLAND OF HAWAII, HAWAII\n",
       "15  2019-10-01    13:35:41.84hr    36.69     97.69                   OKLAHOMA\n",
       "16  2019-10-01    13:11:08.84hr    35.75    117.59        SOUTHERN CALIFORNIA\n",
       "17  2019-10-01    12:32:48.75hr    35.66    117.47        SOUTHERN CALIFORNIA\n",
       "18  2019-10-01    12:22:28.25hr    41.35     19.57                    ALBANIA\n",
       "19  2019-10-01    12:09:47.05hr    37.42     20.65                 IONIAN SEA"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
